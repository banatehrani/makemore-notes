{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a776430b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n",
      "repo root: /Users/home/Developer/github/makemore-notes\n",
      "num words: 32033\n",
      "first 5 words: ['emma', 'olivia', 'ava', 'isabella', 'sophia']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# reproducibility\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# repo root + data path \n",
    "REPO_ROOT = Path.cwd()\n",
    "if (REPO_ROOT / \"data\").exists() is False and (REPO_ROOT.parent / \"data\").exists():\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "\n",
    "data_path = REPO_ROOT / \"data\" / \"names.txt\"\n",
    "words = data_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "print(\"repo root:\", REPO_ROOT)\n",
    "print(\"num words:\", len(words))\n",
    "print(\"first 5 words:\", words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e8c5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 27\n",
      "itos sample: [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd'), (5, 'e'), (6, 'f'), (7, 'g'), (8, 'h'), (9, 'i'), (10, 'j')]\n",
      "splits: 25626 3203 3204\n",
      "sample train words: ['christie', 'edi', 'wallace', 'arieliz', 'aboubacar']\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary + train/dev/test split (reproducible)\n",
    "\n",
    "# vocabulary (same convention: '.' = 0)\n",
    "chars = sorted(set(\"\".join(words)))\n",
    "stoi = {ch: i + 1 for i, ch in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "\n",
    "print(\"vocab_size:\", vocab_size)\n",
    "print(\"itos sample:\", list(itos.items())[:10])\n",
    "\n",
    "# reproducible shuffle + split\n",
    "g_cpu = torch.Generator().manual_seed(1337)\n",
    "words_shuf = words[:]  # copy\n",
    "perm = torch.randperm(len(words_shuf), generator=g_cpu).tolist()\n",
    "words_shuf = [words_shuf[i] for i in perm]\n",
    "\n",
    "n1 = int(0.8 * len(words_shuf))\n",
    "n2 = int(0.9 * len(words_shuf))\n",
    "words_tr = words_shuf[:n1]\n",
    "words_dev = words_shuf[n1:n2]\n",
    "words_te = words_shuf[n2:]\n",
    "\n",
    "print(\"splits:\", len(words_tr), len(words_dev), len(words_te))\n",
    "print(\"sample train words:\", words_tr[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a58dec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtr/Ytr: torch.Size([182508, 3]) torch.Size([182508])\n",
      "Xdev/Ydev: torch.Size([22860, 3]) torch.Size([22860])\n",
      "Xte/Yte: torch.Size([22778, 3]) torch.Size([22778])\n",
      "  0  X='...' -> Y='c'\n",
      "  1  X='..c' -> Y='h'\n",
      "  2  X='.ch' -> Y='r'\n",
      "  3  X='chr' -> Y='i'\n",
      "  4  X='hri' -> Y='s'\n"
     ]
    }
   ],
   "source": [
    "# dataset builder with configurable context length (block_size)\n",
    "\n",
    "block_size = 3  # context length (Karpathy uses 3 early; we'll grow later)\n",
    "\n",
    "def build_dataset(words_list: list[str], block_size: int):\n",
    "    X, Y = [], []\n",
    "    for w in words_list:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + \".\":\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]  # shift window and append\n",
    "    X = torch.tensor(X, dtype=torch.long, device=device)\n",
    "    Y = torch.tensor(Y, dtype=torch.long, device=device)\n",
    "    return X, Y\n",
    "\n",
    "Xtr, Ytr = build_dataset(words_tr, block_size)\n",
    "Xdev, Ydev = build_dataset(words_dev, block_size)\n",
    "Xte, Yte = build_dataset(words_te, block_size)\n",
    "\n",
    "print(\"Xtr/Ytr:\", Xtr.shape, Ytr.shape)\n",
    "print(\"Xdev/Ydev:\", Xdev.shape, Ydev.shape)\n",
    "print(\"Xte/Yte:\", Xte.shape, Yte.shape)\n",
    "\n",
    "# quick sanity peek\n",
    "def decode_ctx(ctx):\n",
    "    return \"\".join(itos[int(i)] for i in ctx)\n",
    "\n",
    "for i in [0, 1, 2, 3, 4]:\n",
    "    print(f\"{i:>3}  X='{decode_ctx(Xtr[i])}' -> Y='{itos[int(Ytr[i])]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2bf53d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num parameters: 52497\n"
     ]
    }
   ],
   "source": [
    "# define a deeper MLP with BatchNorm (no training yet)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# hyperparameters (explicit and easy to tune)\n",
    "n_embed = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator(device=device).manual_seed(1337)\n",
    "\n",
    "# parameters\n",
    "C = torch.randn((vocab_size, n_embed), generator=g, device=device)\n",
    "\n",
    "W1 = torch.randn((block_size * n_embed, n_hidden), generator=g, device=device) * (5/3) / (block_size * n_embed) ** 0.5\n",
    "b1 = torch.zeros((n_hidden,), device=device)\n",
    "\n",
    "W2 = torch.randn((n_hidden, n_hidden), generator=g, device=device) * (5/3) / n_hidden ** 0.5\n",
    "b2 = torch.zeros((n_hidden,), device=device)\n",
    "\n",
    "W3 = torch.randn((n_hidden, vocab_size), generator=g, device=device) * 0.01\n",
    "b3 = torch.zeros((vocab_size,), device=device)\n",
    "\n",
    "# BatchNorm parameters (learned scale + shift)\n",
    "bn_gain = torch.ones((n_hidden,), device=device)\n",
    "bn_bias = torch.zeros((n_hidden,), device=device)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, W3, b3, bn_gain, bn_bias]\n",
    "for p in parameters:\n",
    "    p.requires_grad_(True)\n",
    "\n",
    "print(\"num parameters:\", sum(p.nelement() for p in parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ff5e398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: torch.Size([32, 27]) loss: 3.294424533843994\n",
      "bn_running_mean/std (first 5): [0.07393120229244232, -0.04032566025853157, -0.029199523851275444, -0.029229525476694107, 0.06531120091676712] [1.0036672353744507, 1.0191179513931274, 1.0848532915115356, 1.0036523342132568, 1.0340840816497803]\n"
     ]
    }
   ],
   "source": [
    "# forward pass with BatchNorm (train-mode), plus running stats init\n",
    "\n",
    "# running stats (buffers, not trained by gradient descent)\n",
    "bn_running_mean = torch.zeros((n_hidden,), device=device)\n",
    "bn_running_var = torch.ones((n_hidden,), device=device)\n",
    "\n",
    "bn_momentum = 0.1\n",
    "bn_eps = 1e-5\n",
    "\n",
    "def forward(Xb: torch.Tensor, *, train: bool) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Xb: (B, block_size) long\n",
    "    returns logits: (B, vocab_size)\n",
    "    \"\"\"\n",
    "    global bn_running_mean, bn_running_var\n",
    "\n",
    "    # embeddings\n",
    "    emb = C[Xb]                           # (B, block_size, n_embed)\n",
    "    x = emb.view(emb.shape[0], -1)        # (B, block_size*n_embed)\n",
    "\n",
    "    # layer 1\n",
    "    h1 = x @ W1 + b1                      # (B, n_hidden)\n",
    "    h1 = torch.tanh(h1)\n",
    "\n",
    "    # layer 2 pre-activation\n",
    "    h2_pre = h1 @ W2 + b2                 # (B, n_hidden)\n",
    "\n",
    "    # BatchNorm on h2_pre\n",
    "    if train:\n",
    "        batch_mean = h2_pre.mean(dim=0, keepdim=False)                   # (n_hidden,)\n",
    "        batch_var = h2_pre.var(dim=0, unbiased=False, keepdim=False)     # (n_hidden,)\n",
    "\n",
    "        h2_hat = (h2_pre - batch_mean) / torch.sqrt(batch_var + bn_eps)\n",
    "\n",
    "        # update running stats (no grad)\n",
    "        with torch.no_grad():\n",
    "            bn_running_mean = (1 - bn_momentum) * bn_running_mean + bn_momentum * batch_mean\n",
    "            bn_running_var  = (1 - bn_momentum) * bn_running_var  + bn_momentum * batch_var\n",
    "    else:\n",
    "        h2_hat = (h2_pre - bn_running_mean) / torch.sqrt(bn_running_var + bn_eps)\n",
    "\n",
    "    # scale + shift (learned)\n",
    "    h2 = bn_gain * h2_hat + bn_bias\n",
    "    h2 = torch.tanh(h2)\n",
    "\n",
    "    # output\n",
    "    logits = h2 @ W3 + b3                 # (B, vocab_size)\n",
    "    return logits\n",
    "\n",
    "# quick sanity forward pass\n",
    "B = 32\n",
    "ix = torch.randint(0, Xtr.shape[0], (B,), generator=g, device=device)\n",
    "logits = forward(Xtr[ix], train=True)\n",
    "loss = F.cross_entropy(logits, Ytr[ix])\n",
    "\n",
    "print(\"logits:\", logits.shape, \"loss:\", loss.item())\n",
    "print(\"bn_running_mean/std (first 5):\",\n",
    "      bn_running_mean[:5].tolist(),\n",
    "      torch.sqrt(bn_running_var[:5]).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d94cb50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0/20000  lr=0.1  train=3.3085  dev=3.3079\n",
      "  1000/20000  lr=0.1  train=2.3706  dev=2.3782\n",
      "  2000/20000  lr=0.1  train=2.3264  dev=2.3385\n",
      "  3000/20000  lr=0.1  train=2.2762  dev=2.2885\n",
      "  4000/20000  lr=0.1  train=2.2922  dev=2.3094\n",
      "  5000/20000  lr=0.1  train=2.2549  dev=2.2700\n",
      "  6000/20000  lr=0.1  train=2.2333  dev=2.2508\n",
      "  7000/20000  lr=0.1  train=2.2396  dev=2.2614\n",
      "  8000/20000  lr=0.1  train=2.2337  dev=2.2584\n",
      "  9000/20000  lr=0.1  train=2.2184  dev=2.2403\n",
      " 10000/20000  lr=0.01  train=2.2209  dev=2.2471\n",
      " 11000/20000  lr=0.01  train=2.1487  dev=2.1773\n",
      " 12000/20000  lr=0.01  train=2.1436  dev=2.1732\n",
      " 13000/20000  lr=0.01  train=2.1392  dev=2.1680\n",
      " 14000/20000  lr=0.01  train=2.1381  dev=2.1667\n",
      " 15000/20000  lr=0.01  train=2.1358  dev=2.1644\n",
      " 16000/20000  lr=0.01  train=2.1330  dev=2.1624\n",
      " 17000/20000  lr=0.01  train=2.1293  dev=2.1596\n",
      " 18000/20000  lr=0.01  train=2.1286  dev=2.1596\n",
      " 19000/20000  lr=0.01  train=2.1267  dev=2.1578\n",
      " 20000/20000  lr=0.01  train=2.1258  dev=2.1574\n"
     ]
    }
   ],
   "source": [
    "# training loop (BatchNorm train-mode) + periodic dev evaluation\n",
    "\n",
    "import math\n",
    "\n",
    "def split_loss(X: torch.Tensor, Y: torch.Tensor, batch_size: int = 4096) -> float:\n",
    "    \"\"\"Average loss over a split (uses BN in eval-mode).\"\"\"\n",
    "    losses = []\n",
    "    for start in range(0, X.shape[0], batch_size):\n",
    "        xb = X[start:start+batch_size]\n",
    "        yb = Y[start:start+batch_size]\n",
    "        logits = forward(xb, train=False)\n",
    "        losses.append(F.cross_entropy(logits, yb).detach())\n",
    "    return torch.stack(losses).mean().item()\n",
    "\n",
    "# training hyperparams\n",
    "max_steps = 20_000\n",
    "batch_size = 32\n",
    "eval_interval = 1_000\n",
    "\n",
    "# simple learning-rate schedule (Karpathy-style, but clean)\n",
    "def lr_at(step: int) -> float:\n",
    "    return 0.1 if step < 10_000 else 0.01\n",
    "\n",
    "lossi = []\n",
    "\n",
    "for step in range(max_steps + 1):\n",
    "    # minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g, device=device)\n",
    "    xb, yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # forward (BN in train mode)\n",
    "    logits = forward(xb, train=True)\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "\n",
    "    # backward\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = lr_at(step)\n",
    "    with torch.no_grad():\n",
    "        for p in parameters:\n",
    "            p -= lr * p.grad\n",
    "\n",
    "    # track\n",
    "    if step % eval_interval == 0:\n",
    "        train_loss = split_loss(Xtr, Ytr)\n",
    "        dev_loss = split_loss(Xdev, Ydev)\n",
    "        print(f\"{step:6d}/{max_steps}  lr={lr:.3g}  train={train_loss:.4f}  dev={dev_loss:.4f}\")\n",
    "        lossi.append(dev_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee88dae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- temperature=0.7 ---\n",
      "rayshika\n",
      "bracolten\n",
      "madari\n",
      "samar\n",
      "quinn\n",
      "jacott\n",
      "faon\n",
      "anleann\n",
      "damaris\n",
      "dylani\n",
      "\n",
      "--- temperature=1.0 ---\n",
      "joa\n",
      "taz\n",
      "malan\n",
      "rhin\n",
      "sywadepha\n",
      "kemae\n",
      "alynn\n",
      "amari\n",
      "taer\n",
      "kya\n",
      "\n",
      "--- temperature=1.3 ---\n",
      "scenney\n",
      "bod\n",
      "khyrikzangi\n",
      "zaxi\n",
      "nafelsa\n",
      "briglazaryn\n",
      "chripm\n",
      "ratyn\n",
      "lie\n",
      "donkianori\n"
     ]
    }
   ],
   "source": [
    "# sample names from the trained BN model (eval-mode forward)\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_names(num_samples: int = 20, max_len: int = 30, temperature: float = 1.0):\n",
    "    out = []\n",
    "    for _ in range(num_samples):\n",
    "        context = [0] * block_size\n",
    "        name_chars = []\n",
    "        while True:\n",
    "            x = torch.tensor([context], dtype=torch.long, device=device)     # (1, block_size)\n",
    "            logits = forward(x, train=False)                                 # BN uses running stats\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=1)                             # (1, vocab_size)\n",
    "\n",
    "            ix = torch.multinomial(probs, num_samples=1).item()\n",
    "            if ix == 0:\n",
    "                break\n",
    "\n",
    "            name_chars.append(itos[ix])\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "            if len(name_chars) >= max_len:\n",
    "                break\n",
    "\n",
    "        out.append(\"\".join(name_chars))\n",
    "    return out\n",
    "\n",
    "for t in [0.7, 1.0, 1.3]:\n",
    "    print(f\"\\n--- temperature={t} ---\")\n",
    "    for n in sample_names(num_samples=10, temperature=t):\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba2ae7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final  train loss: 2.1258\n",
      "final    dev loss: 2.1574\n",
      "final   test loss: 2.1506\n"
     ]
    }
   ],
   "source": [
    "# final evaluation (train/dev/test) using BN eval-mode\n",
    "\n",
    "train_loss = split_loss(Xtr, Ytr)\n",
    "dev_loss   = split_loss(Xdev, Ydev)\n",
    "test_loss  = split_loss(Xte, Yte)\n",
    "\n",
    "print(f\"final  train loss: {train_loss:.4f}\")\n",
    "print(f\"final    dev loss: {dev_loss:.4f}\")\n",
    "print(f\"final   test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48d53f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part: 03\n",
      "notebook: 04_mlp_deepening_and_normalization.ipynb\n",
      "device: mps\n",
      "block_size: 3\n",
      "vocab_size: 27\n",
      "n_embed: 10\n",
      "n_hidden: 200\n",
      "max_steps: 20000\n",
      "batch_size: 32\n",
      "lr_schedule: 0.1 for <10k steps, then 0.01\n",
      "batchnorm: {'momentum': 0.1, 'eps': 1e-05, 'running_stats_used_in_eval': True}\n",
      "loss: {'train': 2.125819683074951, 'dev': 2.1573522090911865, 'test': 2.1505839824676514}\n"
     ]
    }
   ],
   "source": [
    "# experiment summary\n",
    "\n",
    "summary = {\n",
    "    \"part\": \"03\",\n",
    "    \"notebook\": \"04_mlp_deepening_and_normalization.ipynb\",\n",
    "    \"device\": str(device),\n",
    "    \"block_size\": block_size,\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"n_embed\": n_embed,\n",
    "    \"n_hidden\": n_hidden,\n",
    "    \"max_steps\": 20_000,\n",
    "    \"batch_size\": 32,\n",
    "    \"lr_schedule\": \"0.1 for <10k steps, then 0.01\",\n",
    "    \"batchnorm\": {\n",
    "        \"momentum\": bn_momentum,\n",
    "        \"eps\": bn_eps,\n",
    "        \"running_stats_used_in_eval\": True,\n",
    "    },\n",
    "    \"loss\": {\n",
    "        \"train\": float(train_loss),\n",
    "        \"dev\": float(dev_loss),\n",
    "        \"test\": float(test_loss),\n",
    "    },\n",
    "}\n",
    "\n",
    "for k, v in summary.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86923972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (makemore-notes)",
   "language": "python",
   "name": "makemore-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
