{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aeb43e3",
   "metadata": {},
   "source": [
    "# 02 â€” Neural bigram model\n",
    "\n",
    "Replace the count-based bigram model with a trainable neural model:\n",
    "- one-hot encoded inputs\n",
    "- linear layer\n",
    "- softmax\n",
    "- cross-entropy loss\n",
    "- gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e946f033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146]), torch.Size([228146]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# load data\n",
    "REPO_ROOT = Path.cwd()\n",
    "if (REPO_ROOT / \"data\").exists() is False and (REPO_ROOT.parent / \"data\").exists():\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "\n",
    "data_path = REPO_ROOT / \"data\" / \"names.txt\"\n",
    "words = data_path.read_text().splitlines()\n",
    "\n",
    "# vocabulary\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "# build dataset\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        xs.append(stoi[ch1])\n",
    "        ys.append(stoi[ch2])\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "xs.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be9981d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# initialize weights\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((vocab_size, vocab_size), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96bd0cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.758953332901001"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encode inputs\n",
    "xenc = F.one_hot(xs, num_classes=vocab_size).float()  # (N, vocab_size)\n",
    "\n",
    "# logits\n",
    "logits = xenc @ W  # (N, vocab_size)\n",
    "\n",
    "# cross-entropy loss (this is the same NLL objective)\n",
    "loss = F.cross_entropy(logits, ys)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f8b2386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0/4000  loss=3.7590\n",
      " 200/4000  loss=2.4648\n",
      " 400/4000  loss=2.4589\n",
      " 600/4000  loss=2.4571\n",
      " 800/4000  loss=2.4563\n",
      "1000/4000  loss=2.4558\n",
      "1200/4000  loss=2.4554\n",
      "1400/4000  loss=2.4552\n",
      "1600/4000  loss=2.4550\n",
      "1800/4000  loss=2.4549\n",
      "2000/4000  loss=2.4548\n",
      "2200/4000  loss=2.4547\n",
      "2400/4000  loss=2.4547\n",
      "2600/4000  loss=2.4546\n",
      "2800/4000  loss=2.4546\n",
      "3000/4000  loss=2.4545\n",
      "3200/4000  loss=2.4545\n",
      "3400/4000  loss=2.4545\n",
      "3600/4000  loss=2.4544\n",
      "3800/4000  loss=2.4544\n",
      "4000/4000  loss=2.4544\n"
     ]
    }
   ],
   "source": [
    "lr = 40.0\n",
    "steps = 4000\n",
    "\n",
    "for i in range(steps+1):\n",
    "    # forward\n",
    "    xenc = F.one_hot(xs, num_classes=vocab_size).float()\n",
    "    logits = xenc @ W\n",
    "    loss = F.cross_entropy(logits, ys)\n",
    "\n",
    "    # backward\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data -= lr * W.grad\n",
    "\n",
    "    if i % 200 == 0:\n",
    "        print(f\"{i:4d}/{steps}  loss={loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe23cbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexze\n",
      "momasurailezitynn\n",
      "konimittain\n",
      "llayn\n",
      "ka\n",
      "da\n",
      "staiyaubrtthrigotai\n",
      "moliellavo\n",
      "ke\n",
      "teda\n"
     ]
    }
   ],
   "source": [
    "def sample_name_neural(W, stoi, itos, g, max_len=30):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    for _ in range(max_len):\n",
    "        x = F.one_hot(torch.tensor([ix]), num_classes=vocab_size).float()  # (1, vocab_size)\n",
    "        logits = x @ W                                                     # (1, vocab_size)\n",
    "        probs = F.softmax(logits, dim=1).squeeze(0)                        # (vocab_size,)\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        if ix == 0:\n",
    "            break\n",
    "        out.append(itos[ix])\n",
    "    return \"\".join(out)\n",
    "\n",
    "g_sample = torch.Generator().manual_seed(2147483647)\n",
    "for _ in range(10):\n",
    "    print(sample_name_neural(W, stoi, itos, g_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a0bbf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (makemore-notes)",
   "language": "python",
   "name": "makemore-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
